import requests
from bs4 import BeautifulSoup
import time

base_url = 'https://www.bloomberg.com/quote/'

# url_1 = base_url + 'BOL:SS'

search_ls = ['BOL:SS', 'EVO:SS', 'HMS:SS', 'HOLMB:SS', 'KAHL:SS', 'NOBINA:SS', 'SAND:SS', 'SKISB:SS', 'VOLVB:SS']
# 初始化一个列表来保存所有的帖子信息：
comments = []

# 首先我们写好抓取网页的函数
def get_html(url):
    try:
        r = requests.get(url,timeout=30)
        # r.raise_for_status()
        #这里我们知道百度贴吧的编码是utf-8，所以手动设置的。爬去其他的页面时建议使用：
        # r.endcodding = r.apparent_endconding()
        # r.encoding='utf-8'
        return r.text
    except:
        return " ERROR "

def get_content(url, share_ls):
    '''
    analyze the webpage
    '''

    # initialize a dict for storing the results
    comments = []

    for share_t in share_ls:
        comment = {}

        url_sub = base_url + share_t

        print(url_sub)

        time.sleep(2)

        try:
            # download the webpage
            html = get_html(url_sub)

            # analyze it use BS4
            soup = BeautifulSoup(html, 'lxml')

            comment['company_name'] = soup.find('h1', attrs={'class': 'companyName__99a4824b'}).text.strip()
            comment['latest_price'] = soup.find('span', attrs={'class': 'priceText__1853e8a5'}).text.strip()

            comments.append(comment)

            # print(comments)

        except:
            print('fail')

    return comments

content = get_content(base_url, search_ls)

print(content)
